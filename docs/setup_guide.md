Installation and test run
======
## Content:
##### 1. [Download Test Data and Reference Data](#1)
##### 2. [Create ```inputs.yaml```](#2)
##### 3. [Run pipeline using conda environment or docker image](#3)
###### 3.1 [Using conda environment](#3.1)
###### 3.2 [Using docker image](#3.2)
##### 4. [Understand the pipeline outputs](#4)
---
## 1. <a name="1"></a>Download Test Data and Reference Data
#### Set up Azure Cli
1. Make sure Azure Client is installed. Else install Azure Client by calling:
```
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash		
```
2. Login to the Azure account
```
az login
```

#### Download reference data:
```
az storage blob download-batch -s refdata -d /refdata --account-name singlecelltestsets --auth-mode login
```
*Make sure the destination path ```/refdata``` already exists. If you don't have permissions to write to ```/refdata``` directory, please feel free to extract it someplace else. Note down the directory where you store the refdata.*

#### Download the test data

Here we will use the test data for alignment part as an example.

```
az storage blob download-batch -s tumourfastq -d /data/ALIGN/tumourfastq --account-name singlecelltestsets --auth-mode login
```

*Make sure the destination path ```/data/ALIGN/tumourfastq``` already exists. If not, call mkdir to create the directory. If you don't have permissions to write to ```/data``` directory, please feel free to extract it someplace else. Note down the directory where you store the test data.*

## <a name="2"></a>2. Create ```inputs.yaml```

  The pipeline takes a yaml formatted input file with paths to fastq files, aligned bam files (will be generated by the pipeline) and metadata. The input yaml file structure is as follows:

  ```
SA123:
 bam: data/SA123.bam
 column: 1
 condition: A
 fastqs:
	 LANE_1:
		 fastq_1: data/SA123_R1.fastq.gz
		 fastq_2: data/SA123_R2.fastq.gz
		 sequencing_center: BCCAGSC
		 sequencing_instrument: HX
 img_col: 2
 index_i5: i5-1
 index_i7: i7-2
 pick_met: A1
 primer_i5: AACCGGTT
 primer_i7: ACGTACGT
 row: 3
 sample_type: null
SA456:
 bam: data/SA456.bam
 column: 4
 condition: A
 fastqs:
	 LANE_1:
		 fastq_1: data/SA456_R1.fastq.gz
		 fastq_2: data/SA456_R2.fastq.gz
		 sequencing_center: BCCAGSC
		 sequencing_instrument: HX
 img_col: 5
 index_i5: i5-1
 index_i7: i7-2
 pick_met: B1
 primer_i5: AACCGGTT
 primer_i7: ACGTACGT
 row: 6
 sample_type: null

  ```
  A sample input yaml file for the data downloaded in step 8 can be found in ```INSTALL/input.yaml```.\
  NOTE: The paths to files in ```INSTALL/input.yaml``` point to the ```/data``` directory. If you extracted the files to another location, please update the file accordingly.

## <a name="3"></a>3. Run pipeline using conda environment or docker image

  ### <a name="3.1"></a> 3.1 Using conda environment
  Note: In this tutorial, we'll use the alignment as an example.

#### Overview:

1. [Clone the repo](#3.1.1)
2. [Set up conda channels](#3.1.2)
3. [Install Dependencies and Biowrappers](#3.1.3)
4. [Build pipeline config file ```config.yaml```](#3.1.4)
5. [Run the pipeline](#3.1.5)

#### <a name="3.1.1"></a>  3.1.1 Clone the single cell pipeline repo

```
export GIT_SSL_NO_VERIFY=1
git clone https://github.com/shahcompbio/single_cell_pipeline.git
```

#### <a name="3.1.2"></a> 3.1.2 Set up conda channels

1. install miniconda
```
wget https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh
sudo bash Miniconda2-latest-Linux-x86_64.sh -b -p /usr/local/miniconda2
echo "export PATH=/usr/local/miniconda2/bin:$PATH" >> ~/.bashrc
source ~/.bashrc
sudo chmod -R 775 /usr/local/miniconda2/
```
2.  update conda
```
conda upgrade conda -y
```
3.  add conda channels
```
conda config --add channels https://conda.anaconda.org/dranew
conda config --add channels 'bioconda'
conda config --add channels 'r'
conda config --add channels 'conda-forge'
conda config --add channels https://conda.anaconda.org/aroth85
conda config --add channels https://conda.anaconda.org/shahcompbio
```

#### <a name="3.1.3"></a> 3.1.3 Install dependencies and biowrappers
For Alignment
1.  Install Dependencies + Biowrappers \
For the alignment part:
```
conda install -c conda-forge cryptography
conda install -c shahcompbio single_cell_pipeline_align
conda update --all
```

#### <a name="3.1.4"></a> 3.1.4 Build pipeline config file
```
single_cell generate_config --pipeline_config config.yaml
```
NOTE: If reference data is not in ```/refdata```, then please update the following values to the correct path in the config.yaml file
```gc_windows: /refdata/gc_windows.txt```
```ref_genome: /refdata/GRCh37-lite.fa```
```
hmmcopy_params:
  autoploidy:
    exclude_list: /refdata/repeats.satellite.regions
```
```
hmmcopy_params:
  autoploidy:
    gc_wig_file: /refdata/GRCh37-lite.gc.ws_500000.wig
```
```
hmmcopy_params:
  autoploidy:
    map_wig_file: /refdata/GRCh37-lite.map.ws_125_to_500000.wig
```

#### <a name="3.1.5"></a> 3.1.5 Run the alignment pipeline:
```
single_cell alignment \
--input_yaml path/to/input.yaml \
--out_dir dlp \
--bams_dir bams \
--submit local \
--nocleanup \
--config_override '{"refdir": "path/to/refdata"}' \
--library_id A96139A \
--loglevel DEBUG \
--tmpdir temp \
--pipelinedir path/to/pipeline \
--maxjobs 2 \
--config_file /path/to/config.yaml
```

Note:  
The paths in ```--out_dir```, ```--bams_dir```, ```--tmpdir``` and ```--pipelinedir``` will be created automatically.

### <a name="3.2"></a> 3.2 Using docker image
Note: Please make sure you have docker installed.
#### 3.2.1 Build ```context_config.yaml```
Example:
```
docker:
    server: 'docker.io'
    username: null
    password: null
    org: singlecellpipelinetest
    mounts:
      home: /path/to/your/working/directory
      refdata: /path/to/refdata

```
#### 3.2.2 Run the alignment pipeline
Please replace the ```$PWD``` below with your current working directory.
```
docker run -w $PWD -v $PWD:$PWD -v /datadrive:/datadrive -v /var/run/docker.sock:/var/run/docker.sock -v /usr/bin/docker:/usr/bin/docker \
--rm singlecellpipeline/single_cell_pipeline:v0.5.17 single_cell alignment \
--input_yaml /home/qfliu/align/inputs.yaml \
--library_id A97318A \
--maxjobs 1 \
--nocleanup \
--sentinel_only  \
--config_override '{"refdir": "/datadrive/refdata"}' \
--context_config /home/qfliu/align/context_config.yaml \
--submit local \
--loglevel DEBUG \
--tmpdir /datadrive/ALIGN/temp \
--pipelinedir /datadrive/ALIGN/pipeline \
--submit local \
--out_dir /datadrive/ALIGN/output \
--bams_dir /datadrive/ALIGN/bams

```
#### 3.2.3 Use Azure blob storage accounts

If you'd like to store and the use the data in Azure storage accounts. Please refer to [azure blob storage tutorial](../docs/azure/blobstorage.md).

---
## <a name="4"></a>Understanding the pipeline outputs:

Please refer to [doc](../../README.md) for detailed instructions for running all single cell sub commands.

Replace ```/path/to/``` with your path to config.yaml

*Outputs:*
* ```data/SA123.bam``` and ```data/SA456.bam```: aligned bam files
* ```dlp/results/alignment/A123456_alignment_metrics.csv.gz```: alignment metrics and metadata
* ```dlp/results/alignment/A123456_gc_metrics.csv.gz```: alignment metrics and metadata
* ```dlp/results/alignment/plots/A123456_plot_metrics.pdf```: alignment QC plots.
* ```dlp/results/hmmcopy_autoploidy/A123456_reads.csv.gz``` with data from hmmcopy
* ```dlp/results/hmmcopy_autoploidy/A123456_metrics.csv.gz``` with data from hmmcopy
* ```dlp/results/hmmcopy_autoploidy/A123456_params.csv.gz``` with data from hmmcopy
* ```dlp/results/hmmcopy_autoploidy/A123456_segments.csv.gz``` with data from hmmcopy
* ```dlp/results/hmmcopy_autoploidy/A123456_igv_segments.seg``` to load segments in IGV
* ```dlp/results/hmmcopy_autoploidy/plots/A123456_heatmap_by_ec.pdf ```: copy number heatmap
* ```dlp/results/hmmcopy_autoploidy/plots/A123456_heatmap_by_ec_filtered.pdf ```: heatmap with filters to remove bad cells
* ```dlp/results/hmmcopy_autoploidy/plots/A123456_kernel_density.pdf ```: kernel density plot
* ```dlp/results/hmmcopy_autoploidy/plots/A123456_metrics.pdf ``` hmmcopy metrics
* ```dlp/results/hmmcopy_autoploidy/plots/A123456_bias.tar.gz ``` GC bias plots
* ```dlp/results/hmmcopy_autoploidy/plots/A123456_segments.tar.gz ``` hmmcopy segments
